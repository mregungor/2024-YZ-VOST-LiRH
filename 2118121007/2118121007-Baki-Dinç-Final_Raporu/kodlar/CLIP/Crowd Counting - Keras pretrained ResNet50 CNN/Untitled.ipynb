{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "896f2e5e-33ff-4672-aaa1-cdf7f2ef1f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.007959365844726562 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "__author__ = \"Thai Thien\"\n",
    "__email__ = \"tthien@apcs.vn\"\n",
    "\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "import h5py\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "__DATASET_ROOT = r\"C:/Users/bakiD/OneDrive/Masaüstü/images/Yeni klasör (6)/ShanghaiTech/part_B/train_data\"\n",
    "__OUTPUT_NAME = \"ShanghaiTech_PartB_Train/\"\n",
    "\n",
    "\n",
    "def gaussian_filter_density(gt):\n",
    "    print(gt.shape)\n",
    "    density = np.zeros(gt.shape, dtype=np.float32)\n",
    "    gt_count = np.count_nonzero(gt)\n",
    "    if gt_count == 0:\n",
    "        return density\n",
    "\n",
    "    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n",
    "    leafsize = 2048\n",
    "    # build kdtree\n",
    "    pts_copy = pts.copy()\n",
    "    tree = scipy.spatial.KDTree(pts_copy, leafsize=leafsize)\n",
    "    # query kdtree\n",
    "    distances, locations = tree.query(pts, k=4)\n",
    "\n",
    "    print('generate density...')\n",
    "    for i, pt in enumerate(pts):\n",
    "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
    "        pt2d[pt[1], pt[0]] = 1.\n",
    "        if gt_count > 1:\n",
    "            sigma = (distances[i][1] + distances[i][2] + distances[i][3]) * 0.1\n",
    "        else:\n",
    "            sigma = np.average(np.array(gt.shape)) / 2. / 2.  # case: 1 point\n",
    "        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')\n",
    "    print('done.')\n",
    "    return density\n",
    "\n",
    "\n",
    "def single_sample_prototype():\n",
    "    img_path = '/data/dump/ShanghaiTech/part_A/train_data/images/IMG_2.jpg'\n",
    "    print(img_path)\n",
    "    mat_path = \"/data/dump/ShanghaiTech/part_A/train_data/ground-truth/GT_IMG_2.mat\"\n",
    "    mat = scipy.io.loadmat(mat_path)\n",
    "    imgfile = image.load_img(img_path)\n",
    "    img = image.img_to_array(imgfile)\n",
    "    k = np.zeros((img.shape[0], img.shape[1]))\n",
    "    gt = mat[\"image_info\"][0, 0][0, 0][0]\n",
    "    for i in range(0, len(gt)):\n",
    "        if int(gt[i][1]) < img.shape[0] and int(gt[i][0]) < img.shape[1]:\n",
    "            k[int(gt[i][1]), int(gt[i][0])] = 1\n",
    "    k = gaussian_filter_density(k)\n",
    "\n",
    "\n",
    "def generate_density_map(img_path):\n",
    "    print(img_path)\n",
    "    mat_path = img_path.replace('.jpg', '.mat').replace('images', 'ground-truth').replace('IMG_', 'GT_IMG_')\n",
    "    mat = scipy.io.loadmat(mat_path)\n",
    "    imgfile = image.load_img(img_path)\n",
    "    img = image.img_to_array(imgfile)\n",
    "    k = np.zeros((img.shape[0], img.shape[1]))\n",
    "    gt = mat[\"image_info\"][0, 0][0, 0][0]\n",
    "    for i in range(0, len(gt)):\n",
    "        if int(gt[i][1]) < img.shape[0] and int(gt[i][0]) < img.shape[1]:\n",
    "            k[int(gt[i][1]), int(gt[i][0])] = 1\n",
    "    k = gaussian_filter_density(k)\n",
    "    output_path = img_path.replace(__DATASET_ROOT, __OUTPUT_NAME).replace('.jpg', '.h5').replace('images','ground-truth-h5')\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"output\", output_path)\n",
    "    with h5py.File(output_path, 'w') as hf:\n",
    "        hf['density'] = k\n",
    "    return img_path\n",
    "\n",
    "\n",
    "def generate_shanghaitech_path(root):\n",
    "    # now generate the ShanghaiA's ground truth\n",
    "    part_A_train = os.path.join(root, 'part_A/train_data', 'images')\n",
    "    part_A_test = os.path.join(root, 'part_A/test_data', 'images')\n",
    "    part_B_train = os.path.join(root, 'part_B/train_data', 'images')\n",
    "    part_B_test = os.path.join(root, 'part_B/test_data', 'images')\n",
    "    path_sets = [part_A_train, part_A_test, part_B_train, part_B_test]\n",
    "\n",
    "    img_paths_a_train = []\n",
    "    img_paths_a_test = []\n",
    "    img_paths_b_train = []\n",
    "    img_paths_b_test = []\n",
    "\n",
    "    for img_path in glob.glob(os.path.join(part_A_train, '*.jpg')):\n",
    "        img_paths_a_train.append(img_path)\n",
    "    for img_path in glob.glob(os.path.join(part_B_train, '*.jpg')):\n",
    "        img_paths_b_train.append(img_path)\n",
    "\n",
    "    for img_path in glob.glob(os.path.join(part_A_test, '*.jpg')):\n",
    "        img_paths_a_test.append(img_path)\n",
    "    for img_path in glob.glob(os.path.join(part_B_test, '*.jpg')):\n",
    "        img_paths_b_test.append(img_path)\n",
    "\n",
    "    return img_paths_a_train, img_paths_a_test, img_paths_b_train, img_paths_b_test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    TODO: this file will preprocess crowd counting dataset\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    a_train, a_test, b_train, b_test = generate_shanghaitech_path(__DATASET_ROOT)\n",
    "\n",
    "    Parallel(n_jobs=4)(delayed(generate_density_map)(p) for p in b_train)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fd329-2cff-4378-a029-4ccf48a2e125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d562e6-ed67-4019-80c2-43587dfd6716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
