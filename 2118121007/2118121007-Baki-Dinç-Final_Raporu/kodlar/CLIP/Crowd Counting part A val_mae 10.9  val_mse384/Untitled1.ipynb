{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521aba38-57fe-417c-bf71-1af55780ebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\bakid\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from keras) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from optree->keras) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bakid\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9b0b1e-8347-47b0-9c94-c1510a7d15f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pykdtree in c:\\users\\bakid\\anaconda3\\lib\\site-packages (1.3.11)\n",
      "Requirement already satisfied: numpy in c:\\users\\bakid\\anaconda3\\lib\\site-packages (from pykdtree) (1.26.4)\n",
      "Requirement already satisfied: kdtree in c:\\users\\bakid\\anaconda3\\lib\\site-packages (0.16)\n"
     ]
    }
   ],
   "source": [
    "#Definition of library\n",
    "!pip install pykdtree\n",
    "!pip install kdtree\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import argparse\n",
    "from scipy.io import loadmat\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from pykdtree.kdtree import KDTree\n",
    "import glob\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import scipy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from utils_gen import gen_density_map_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a77c90-5e3c-4de7-99bc-52ac5b50fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_density_map(img, anno_points):\n",
    "    # Görüntü yoksa veya boşsa hata döndür\n",
    "    if img is None or img.size == 0:\n",
    "        raise ValueError(\"Image is None or empty.\")\n",
    "\n",
    "    # Görüntü boyutunu al\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Yoğunluk haritasını oluştur\n",
    "    density_map = np.zeros((h, w), dtype=np.float64)\n",
    "    kernel_size = 15  # Gaussian kernel boyutu\n",
    "    sigma = 4.0  # standart sapma\n",
    "\n",
    "    for point in anno_points:\n",
    "        # İnsan başının merkez noktası koordinatları\n",
    "        x, y = min(w - 1, abs(math.floor(point[0]))), min(h - 1, abs(math.floor(point[1])))\n",
    "        # Üst sol köşe koordinatları ve alt sağ köşe koordinatları\n",
    "        x1, y1 = max(0, x - kernel_size // 2), max(0, y - kernel_size // 2)\n",
    "        x2, y2 = min(w, x + kernel_size // 2 + 1), min(h, y + kernel_size // 2 + 1)\n",
    "\n",
    "        # Sınırlar dışına çıkılıp çıkılmadığını kontrol et\n",
    "        out_of_bounds = False\n",
    "        dx1, dy1, dx2, dy2 = 0, 0, 0, 0  # Sınırlar dışı ofset\n",
    "        if x1 < 0:\n",
    "            dx1 = abs(x1)\n",
    "            x1 = 0\n",
    "            out_of_bounds = True\n",
    "        if y1 < 0:\n",
    "            dy1 = abs(y1)\n",
    "            y1 = 0\n",
    "            out_of_bounds = True\n",
    "        if x2 > w:\n",
    "            dx2 = x2 - w\n",
    "            x2 = w\n",
    "            out_of_bounds = True\n",
    "        if y2 > h:\n",
    "            dy2 = y2 - h\n",
    "            y2 = h\n",
    "            out_of_bounds = True\n",
    "\n",
    "        if out_of_bounds:\n",
    "            # Sınırlar dışına çıkıldığında, Gauss çekirdeğinin boyutunu ayarla\n",
    "            kernel_h = kernel_size - dy1 - dy2\n",
    "            kernel_w = kernel_size - dx1 - dx2\n",
    "            # (kernel_h, kernel_w) boyutunda bir Gauss çekirdeği oluştur\n",
    "            H = np.multiply(cv2.getGaussianKernel(kernel_h, sigma), (cv2.getGaussianKernel(kernel_w, sigma)).T)\n",
    "        else:\n",
    "            # (15, 15) boyutunda bir Gauss çekirdeği oluştur\n",
    "            H = np.multiply(cv2.getGaussianKernel(kernel_size, sigma), (cv2.getGaussianKernel(kernel_size, sigma)).T)\n",
    "\n",
    "        density_map[y1:y2, x1:x2] += H\n",
    "\n",
    "    return density_map\n",
    "\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data_path, gt_path, shuffle=False, gt_downsample=False):\n",
    "     \n",
    "        self.data_path = data_path\n",
    "        self.gt_path = gt_path\n",
    "        self.shuffle = shuffle\n",
    "        self.gt_downsample = gt_downsample\n",
    "        self.data_files = [filename for filename in os.listdir(data_path)]\n",
    "        self.num_samples = len(self.data_files)\n",
    "        self.blob_list = []\n",
    "\n",
    "        for fname in self.data_files:\n",
    "            img = cv2.imread(os.path.join(self.data_path, fname), 0)\n",
    "            img = img.astype(np.float32, copy=False)\n",
    "            ht = img.shape[0]\n",
    "            wd = img.shape[1]\n",
    "            ht_1 = int((ht / 4) * 4)\n",
    "            wd_1 = int((wd / 4) * 4)\n",
    "            img = cv2.resize(img, (wd_1, ht_1))\n",
    "            img = img.reshape((img.shape[0], img.shape[1], 1))\n",
    "            den = pd.read_csv(os.path.join(self.gt_path, os.path.splitext(fname)[0] +'.csv'),\n",
    "                              header=None).values\n",
    "            den = den.astype(np.float32, copy=False)\n",
    "            if self.gt_downsample:\n",
    "                wd_1 = int(wd_1 / 4)\n",
    "                ht_1 = int(ht_1 / 4)\n",
    "            den = cv2.resize(den, (wd_1, ht_1))\n",
    "            den = den * ((wd * ht) / (wd_1 * ht_1))\n",
    "            den = den.reshape((den.shape[0], den.shape[1], 1))\n",
    "\n",
    "            blob = dict()\n",
    "            blob['data'] = img\n",
    "            blob['gt'] = den\n",
    "            blob['fname'] = fname\n",
    "            self.blob_list.append(blob)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.blob_list)\n",
    "\n",
    "    def flow(self, batch_size=32):\n",
    "        loop_count = self.num_samples // batch_size\n",
    "        while True:\n",
    "            np.random.shuffle(self.blob_list)\n",
    "            for i in range(loop_count):\n",
    "                blobs = self.blob_list[i*batch_size: (i+1)*batch_size]\n",
    "                X_batch = np.array([blob['data'] for blob in blobs])\n",
    "                Y_batch = np.array([blob['gt'] for blob in blobs])\n",
    "                yield X_batch, Y_batch\n",
    "\n",
    "    def get_all(self):\n",
    "        X = np.array([blob['data'] for blob in self.blob_list])\n",
    "        Y = np.array([blob['gt'] for blob in self.blob_list])\n",
    "        return X, Y\n",
    "\n",
    "    def __iter__(self):\n",
    "        for blob in self.blob_list:\n",
    "            yield blob\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return K.abs(K.sum(y_true) - K.sum(y_pred))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return (K.sum(y_true) - K.sum(y_pred)) * (K.sum(y_true) - K.sum(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685973cc-7a69-40cc-bdf7-98d32bf84bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bakiD\\OneDrive\\Masaüstü\\Shangtai\\part_A\\train_data\\ground-truthGT_IMG_1.mat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "EPOCHS = 200\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VAL_BATCH_SIZE = 1\n",
    "dataset = 'A'\n",
    "MODEL_DIR = './trained_models/'\n",
    "\n",
    "TRAIN_PATH = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\ShanghaiTech\\part_A\\train_data\\images\"\n",
    "TRAIN_GT_PATH = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\ShanghaiTech\\part_A\\train_data\\ground-truth\"\n",
    "\n",
    "VAL_PATH = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\ShanghaiTech\\part_A\\val_data\\images\"\n",
    "VAL_GT_PATH = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\ShanghaiTech\\part_A\\val_data\\ground-truth\"\n",
    "\n",
    "TEST_PATH = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\ShanghaiTech\\part_A\\test_data\\images\"\n",
    "TEST_GT_PATH = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\Shangtai\\part_A\\test_data\\ground-truth\"\n",
    "\n",
    "HM_GT_PATH = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\Shangtai\\part_A\\Heat_map\"\n",
    "\n",
    "path = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\Shangtai\\part_A\\train_data\\images\"\n",
    "gt_path = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\Shangtai\\part_A\\train_data\\ground-truth\"\n",
    "gt_path_csv = r\"C:\\Users\\bakiD\\OneDrive\\Masaüstü\\Shangtai\\part_A/ground_truth_csv/\"\n",
    "\n",
    "if not os.path.exists(gt_path_csv):\n",
    "    os.makedirs(gt_path_csv)\n",
    "#num image\n",
    "num_images = 300\n",
    "x = ''.join((gt_path, 'GT_IMG_1', '.mat'))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01b4131-a482-4ab7-bf8d-48b7e0ec6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10/300 files\n",
      "Processing 20/300 files\n",
      "Processing 30/300 files\n",
      "Processing 40/300 files\n",
      "Processing 50/300 files\n",
      "Processing 60/300 files\n",
      "Processing 70/300 files\n",
      "Processing 80/300 files\n",
      "Processing 90/300 files\n",
      "Processing 100/300 files\n",
      "Processing 110/300 files\n",
      "Processing 120/300 files\n",
      "Processing 130/300 files\n",
      "Processing 140/300 files\n",
      "Processing 150/300 files\n",
      "Processing 160/300 files\n",
      "Processing 170/300 files\n",
      "Processing 180/300 files\n",
      "Processing 190/300 files\n",
      "Processing 200/300 files\n",
      "Processing 210/300 files\n",
      "Processing 220/300 files\n",
      "Processing 230/300 files\n",
      "Processing 240/300 files\n",
      "Processing 250/300 files\n",
      "Processing 260/300 files\n",
      "Processing 270/300 files\n",
      "Processing 280/300 files\n",
      "Processing 290/300 files\n",
      "Processing 300/300 files\n"
     ]
    }
   ],
   "source": [
    "num_val = math.ceil(num_images * 0.1)  \n",
    "indices = list(range(1, num_images + 1))\n",
    "random.shuffle(indices)\n",
    "\n",
    "for idx in range(num_images):\n",
    "\n",
    "\n",
    "    i = indices[idx]\n",
    "    if (idx+1) % 10 == 0:\n",
    "        print('Processing {}/{} files'.format(idx+1, num_images))\n",
    "    \n",
    "    input_img_name = ''.join((path, 'IMG_', str(i), '.jpg'))\n",
    "    if os.path.isfile(input_img_name):\n",
    "\n",
    "      im = cv2.imread(input_img_name, 0)\n",
    "      \n",
    "      image_info = loadmat(''.join((gt_path, 'GT_IMG_', str(i), '.mat')))['image_info']\n",
    "      annPoints = image_info[0][0][0][0][0] - 1\n",
    "      \n",
    "      im_density = gen_density_map.gen_density_map(im, annPoints)\n",
    "\n",
    "      h, w = im.shape\n",
    "      wn2, hn2 = w / 8, h / 8  \n",
    "      wn2, hn2 = int(wn2 / 8) * 8, int(hn2 / 8) * 8  \n",
    "      \n",
    "      xmin, xmax = wn2, w - wn2\n",
    "      ymin, ymax = hn2, h - hn2\n",
    "      \n",
    "      for j in range(1, N + 1):\n",
    "          \n",
    "          x = math.floor((xmax - xmin) * random.random() + xmin)\n",
    "          y = math.floor((ymax - ymin) * random.random() + ymin)\n",
    "          \n",
    "          x1, y1 = x - wn2, y - hn2\n",
    "          x2, y2 = x + wn2, y + hn2\n",
    "          \n",
    "          im_sampled = im[y1:y2, x1:x2]\n",
    "          im_density_sampled = im_density[y1:y2, x1:x2]\n",
    "\n",
    "          \n",
    "          img_idx = ''.join((str(i), '_', str(j)))\n",
    "          path_img, path_den = (val_path_img, val_path_den) if (idx+1) < num_val else (train_path_img, train_path_den)\n",
    "          cv2.imwrite(''.join([path_img, img_idx, '.jpg']), im_sampled)\n",
    "          with open(''.join([path_den, img_idx, '.csv']), 'w', newline='') as fout:\n",
    "              writer = csv.writer(fout)\n",
    "              writer.writerows(im_density_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffdd574b-77ae-4418-acb5-66986607cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShanghaiTech/part_A/train_data/ground-truth/\n"
     ]
    }
   ],
   "source": [
    "dataset = 'A'\n",
    "N = 9  \n",
    "dataset_name = ''.join(['ShanghaiTech_part_', dataset, '_patches_', str(N)])\n",
    "path = ''.join(['ShanghaiTech/part_', dataset, '/train_data/images/'])\n",
    "output_path = 'data/formatted_trainval_{}/'.format(dataset)\n",
    "train_path_img = ''.join((output_path, dataset_name, '/train/'))\n",
    "train_path_den = ''.join((output_path, dataset_name, '/train_den/'))\n",
    "val_path_img = ''.join((output_path, dataset_name, '/val/'))\n",
    "val_path_den = ''.join((output_path, dataset_name, '/val_den/'))\n",
    "gt_path = ''.join(['ShanghaiTech/part_', dataset, '/train_data/ground-truth/'])\n",
    "print(gt_path)\n",
    "\n",
    "for i in [output_path, train_path_img, train_path_den, val_path_img, val_path_den]:\n",
    "    if not os.path.exists(i):\n",
    "        os.makedirs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd5ca1e2-0236-4360-8167-1530bec6d8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/bakiD/OneDrive/Masaüstü/ShanghaiTech/part_A/train_data/ground-truth/\n",
      "()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image is None or empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m density_map \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(im, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(density_map\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 48\u001b[0m im_density \u001b[38;5;241m=\u001b[39m \u001b[43mgen_density_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannPoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m h, w \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     51\u001b[0m wn2, hn2 \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m, h \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m  \n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mgen_density_map\u001b[1;34m(img, anno_points)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_density_map\u001b[39m(img, anno_points):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Görüntü yoksa veya boşsa hata döndür\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m img\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage is None or empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Görüntü boyutunu al\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Image is None or empty."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 26\n",
    "random.seed(seed)\n",
    "dataset = 'A'\n",
    "N = 9  \n",
    "dataset_name = ''.join(['ShanghaiTech_part_', dataset, '_patches_', str(N)])\n",
    "path = r\"C:/Users/bakiD/OneDrive/Masaüstü/ShanghaiTech/part_A/train_data/images/\"\n",
    "output_path = 'data/formatted_trainval_{}/'.format(dataset)\n",
    "train_path_img = ''.join((output_path, dataset_name, '/train/'))\n",
    "train_path_den = ''.join((output_path, dataset_name, '/train_den/'))\n",
    "val_path_img = ''.join((output_path, dataset_name, '/val/'))\n",
    "val_path_den = ''.join((output_path, dataset_name, '/val_den/'))\n",
    "gt_path = r\"C:/Users/bakiD/OneDrive/Masaüstü/ShanghaiTech/part_A/train_data/ground-truth/\"\n",
    "print(gt_path)\n",
    "for i in [output_path, train_path_img, train_path_den, val_path_img, val_path_den]:\n",
    "    if not os.path.exists(i):\n",
    "        os.makedirs(i)\n",
    "\n",
    "if dataset == 'A':\n",
    "    num_images = 300\n",
    "else:\n",
    "    num_images = 400\n",
    "\n",
    "num_val = math.ceil(num_images * 0.1) \n",
    "indices = list(range(1, num_images + 1))\n",
    "random.shuffle(indices)\n",
    "\n",
    "for idx in range(num_images):\n",
    "\n",
    "  i = indices[idx]\n",
    "  if (idx+1) % 10 == 0:\n",
    "      print('Processing {}/{} files'.format(idx+1, num_images))\n",
    "  \n",
    "  input_img_name = ''.join((path, 'IMG_', str(i), '.jpg'))\n",
    "  im = cv2.imread(input_img_name, 0)  \n",
    "\n",
    "\n",
    "  \n",
    "  x11=''.join((gt_path, 'GT_IMG_', str(i), '.mat'))\n",
    "\n",
    "  image_info = loadmat(''.join((gt_path, 'GT_IMG_', str(i), '.mat')))['image_info']\n",
    "  annPoints = image_info[0][0][0][0][0] - 1\n",
    "\n",
    "  density_map = np.zeros_like(im, dtype=np.float64)\n",
    "  print(density_map.shape)\n",
    "  \n",
    "  im_density = gen_density_map(im, annPoints)\n",
    "  \n",
    "  h, w = im.shape\n",
    "  wn2, hn2 = w / 8, h / 8  \n",
    "  wn2, hn2 = int(wn2 / 8) * 8, int(hn2 / 8) * 8 \n",
    "  xmin, xmax = wn2, w - wn2\n",
    "  ymin, ymax = hn2, h - hn2\n",
    "  for j in range(1, N + 1):\n",
    "      x = math.floor((xmax - xmin) * random.random() + xmin)\n",
    "      y = math.floor((ymax - ymin) * random.random() + ymin)\n",
    "      x1, y1 = x - wn2, y - hn2\n",
    "      x2, y2 = x + wn2, y + hn2\n",
    "     \n",
    "      im_sampled = im[y1:y2, x1:x2]\n",
    "      im_density_sampled = im_density[y1:y2, x1:x2]\n",
    "     \n",
    "      img_idx = ''.join((str(i), '_', str(j)))\n",
    "      path_img, path_den = (val_path_img, val_path_den) if (idx+1) < num_val else (train_path_img, train_path_den)\n",
    "      cv2.imwrite(''.join([path_img, img_idx, '.jpg']), im_sampled)\n",
    "      with open(''.join([path_den, img_idx, '.csv']), 'w', newline='') as fout:\n",
    "          writer = csv.writer(fout)\n",
    "          writer.writerows(im_density_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db17a78-b1d2-4a03-b964-edf1dbed3875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
